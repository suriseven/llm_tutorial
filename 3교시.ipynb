{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uKLnslVtHbg"
      },
      "source": [
        "## ***ollama python 라이브러리 설치***\n",
        "- pip 명령을 이용해서 python 라이브러리를 설치합니다\n",
        "- 우리는 ollama라는 python 라이브러리를 이용해서 우리의 프로그램과 LLM을 연결할 것입니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSk57LTFOtP7",
        "outputId": "f6f517de-b6a4-41fc-eab0-6b04e404c0dd"
      },
      "outputs": [],
      "source": [
        "# 만약 Google colab환경에서 실행중이라면 아래와 같이 리눅스용 ollama를 설치합니다\n",
        "# ! curl -fsSL https://ollama.com/install.sh | sh\n",
        "#\n",
        "# 그리고 Google colab의 좌측 하단의 터미널을 열고 아래와 같이 명령을 입력합니다\n",
        "# ollama serve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrVX_1hhn0ru",
        "outputId": "220e3703-9e16-44b9-bd3c-9e62e1e04259"
      },
      "outputs": [],
      "source": [
        "! pip install ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwP-1myJOgnL"
      },
      "source": [
        "### ***언어모델 다운로드 - 1***\n",
        "- gemma 이라는 언어모델을 다운로드 합니다\n",
        "- 모델의 규모는 1b 입니다\n",
        "- 10억개의 파라미터를 가진 모델 입니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrQFB0H1E6kJ",
        "outputId": "0c04b5a8-97d7-4135-e919-508449834ebc"
      },
      "outputs": [],
      "source": [
        "! ollama pull gemma3:1b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vTK8w3tOgnN"
      },
      "source": [
        "### ***언어모델 다운로드 - 2***\n",
        "- llama3.2 라는 언어모델을 다운로드 합니다\n",
        "- 모델의 규모는 1b 입니다\n",
        "- 10억개의 파라미터를 가진 모델 입니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-qxa5JOOgnN",
        "outputId": "b985bcfe-0fe9-444b-cee6-7c55bea3a6e2"
      },
      "outputs": [],
      "source": [
        "! ollama pull llama3.2:1b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k092hgicEzNB"
      },
      "source": [
        "## ***각 모델 테스트***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQOZZI0A2jnU"
      },
      "source": [
        "- ollama 패키지의 chat 함수를 이용해서 LLM과 통신합니다\n",
        "- LLM의 응답에서 생성된 문장 내용을 추출하여 출력합니다\n",
        "- llama3.2:1b와 과 qwen:0.5b 모두 동일하게 실행해봅니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVeU9-RlnuLU",
        "outputId": "91fd4128-80fe-434d-fa8f-b97332aff4cc"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "model='llama3.2:1b'\n",
        "# model='gemma3:1b'\n",
        "\n",
        "messages = [{\n",
        "        'role': 'user',\n",
        "        'content': \"Tell me this situation as a simple one word, 'I got A+ in my Math class!'\"\n",
        "    }]\n",
        "\n",
        "response = ollama.chat(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    options={\n",
        "        'temperature': 0.8\n",
        "    })\n",
        "content = response['message']['content']\n",
        "\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNb9WFs3EzND",
        "outputId": "802470d4-a2e9-47aa-9400-aeceefa9f0de"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "model='gemma3:1b'\n",
        "\n",
        "messages = [{\n",
        "        'role': 'user',\n",
        "        'content': \"Tell me this situation as a simple one word, 'I got A+ in my Math class!'\"\n",
        "    }]\n",
        "\n",
        "response = ollama.chat(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    options={\n",
        "        'temperature': 0.8\n",
        "    })\n",
        "content = response['message']['content']\n",
        "\n",
        "print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSEadY4s8C-C"
      },
      "source": [
        "## ***세 언어모델간의 대화***\n",
        "- llama3.2:1b, gemma3:1b, qwen3:0.6b 모델이 서로 10번 대화를 주고 받습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GRimrcW3KnS",
        "outputId": "439236cd-5e65-4849-c831-caa4e518639e"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "# 아래와 같이 2개의 모델을 사용합니다\n",
        "model_1 = \"gemma3:1b\"\n",
        "model_2 = \"llama3.2:1b\"\n",
        "model_3 = \"qwen3:0.6b\"\n",
        "\n",
        "# model1 에게 먼저 대화를 시작합니다\n",
        "message_from_model_1 = \"\"\n",
        "message_from_model_me = \"Now, let's have an argument with anything. Any topic OK but one sentence per one turn. If you or I tell 'I lost' then I or you win\"\n",
        "\n",
        "print(f\"Me: {message_from_model_me}\")\n",
        "response_3 = ollama.chat(\n",
        "    model=model_3,\n",
        "    messages=[{\"role\": \"user\", \"content\": message_from_model_me}],\n",
        "    options={\n",
        "        'temperature': 0.8\n",
        "    })\n",
        "message_from_model_3 = response_3['message']['content']\n",
        "\n",
        "\n",
        "# 대화를 10번 반복합니다\n",
        "for i in range(10):\n",
        "    # model1의 응답을 받아 모델2가 말하고 이를 모델1에게 전달합니다\n",
        "    print(f\"Model 3 ({model_3}): {message_from_model_3}\")\n",
        "    response_2 = ollama.chat(\n",
        "        model=model_2,\n",
        "        messages=[{\"role\": \"user\", \"content\": message_from_model_3}],\n",
        "        options={\n",
        "            'temperature': 0.8\n",
        "        })\n",
        "    message_from_model_2 = response_2['message']['content']  # model2의 응답을 받습니다\n",
        "\n",
        "    print(\"\\n===============================\")\n",
        "\n",
        "    # model2의 응답을 받아 모델2가 말하고 이를 모델1에게 전달합니다\n",
        "    print(f\"Model 2 ({model_2}): {message_from_model_2}\")\n",
        "    response_1 = ollama.chat(\n",
        "        model=model_1,\n",
        "        messages=[{\"role\": \"user\", \"content\": message_from_model_2}],\n",
        "        options={\n",
        "            'temperature': 0.8\n",
        "        })\n",
        "    message_from_model_1 = response_1['message']['content']  # model1의 응답을 받습니다\n",
        "\n",
        "    print(f\"Model 1 ({model_1}): {message_from_model_1}\")\n",
        "\n",
        "    response_3 = ollama.chat(\n",
        "        model=model_3,\n",
        "        messages=[{\"role\": \"user\", \"content\": message_from_model_1}],\n",
        "        options={\n",
        "            'temperature': 0.8\n",
        "        })\n",
        "    message_from_model_3 = response_3['message']['content']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei0h_F4QOgnT"
      },
      "source": [
        "## ***한글로 시도***\n",
        "- 시작 언어를 한글로 바꿔서 다시 시도해봅니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYU-IKtxOgnU",
        "outputId": "86577919-18e4-4f46-ccd2-adf140e547f1"
      },
      "outputs": [],
      "source": [
        "import ollama\n",
        "\n",
        "# 아래와 같이 2개의 모델을 사용합니다\n",
        "model_1 = \"gemma3:1b\"\n",
        "model_2 = \"llama3.2:1b\"\n",
        "\n",
        "# model1 에게 먼저 대화를 시작합니다\n",
        "message_from_model_1 = \"\"\n",
        "message_from_me = \"오늘 점심 뭐먹지?\"\n",
        "\n",
        "# 초기 메시지를 model1에게 전달합니다\n",
        "print(f\"Me: {message_from_me}\")\n",
        "response_1 = ollama.chat(\n",
        "    model=model_1,\n",
        "    messages=[{\"role\": \"user\", \"content\": message_from_me}],\n",
        "    options={\n",
        "        'temperature': 0.8\n",
        "    })\n",
        "message_from_model_1 = response_1['message']['content']  # model1의 응답을 받습니다\n",
        "\n",
        "# 대화를 10번 반복합니다\n",
        "for i in range(10):\n",
        "    # model1의 응답을 받아 모델2가 말하고 이를 모델1에게 전달합니다\n",
        "    print(f\"Model 1 ({model_1}): {message_from_model_1}\")\n",
        "    response_2 = ollama.chat(\n",
        "        model=model_2,\n",
        "        messages=[{\"role\": \"user\", \"content\": message_from_model_1}],\n",
        "        options={\n",
        "            'temperature': 0.8\n",
        "        })\n",
        "    message_from_model_2 = response_2['message']['content']  # model2의 응답을 받습니다\n",
        "\n",
        "    print(\"\\n===============================\")\n",
        "\n",
        "    # model2의 응답을 받아 모델2가 말하고 이를 모델1에게 전달합니다\n",
        "    print(f\"Model 2 ({model_2}): {message_from_model_2}\")\n",
        "    response_1 = ollama.chat(\n",
        "        model=model_1,\n",
        "        messages=[{\"role\": \"user\", \"content\": message_from_model_2}],\n",
        "        options={\n",
        "            'temperature': 0.8\n",
        "        })\n",
        "    message_from_model_1 = response_1['message']['content']  # model1의 응답을 받습니다"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
